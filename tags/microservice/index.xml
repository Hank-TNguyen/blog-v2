<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>microservice on hanknguyen</title>
    <link>/tags/microservice.html</link>
    <description>Recent content in microservice on hanknguyen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>hanknguyen {year}</copyright>
    <lastBuildDate>Mon, 09 Oct 2023 15:52:20 -0700</lastBuildDate><atom:link href="/tags/microservice/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Use Client Load Balancing to avoid Single Points of Failure</title>
      <link>/posts/client-load-balance.html</link>
      <pubDate>Mon, 09 Oct 2023 15:52:20 -0700</pubDate>
      
      <guid>/posts/client-load-balance.html</guid>
      <description>Service Availability and Single Point of Failure Earlier this year, my boss asked me to evaluate blast radius of outages and if there were single point of failures in our workflow. I found for my service, both outbound and inbound requests, every interaction with other services is a single point of failure. The standard term in micro-service world is Inter-Process Communication (IPC), but my team doesn&amp;rsquo;t adopt the term nor care for other standard terms.</description>
      <content>&lt;h1 id=&#34;service-availability-and-single-point-of-failure&#34;&gt;Service Availability and Single Point of Failure&lt;/h1&gt;
&lt;p&gt;Earlier this year, my boss asked me to evaluate blast radius of outages and if there were single point of failures in our workflow. I found for my service, both outbound and inbound requests, every interaction with other services is a single point of failure. The standard term in micro-service world is Inter-Process Communication (IPC), but my team doesn&amp;rsquo;t adopt the term nor care for other standard terms. Sometimes I feel like my team is somewhat out of touch with the rest of the world, but then again, I think my whole company is somewhat like that. That, however, is not the topic for this post.&lt;/p&gt;
&lt;h2 id=&#34;terminologies&#34;&gt;Terminologies&lt;/h2&gt;
&lt;h3 id=&#34;single-point-of-failure&#34;&gt;Single point of failure&lt;/h3&gt;
&lt;details&gt;
    &lt;summary&gt;Definition of A single point of failure&lt;/summary&gt;
    &lt;p&gt;A Single Point of Failure (SPOF) is a critical element within a system or network, and if it experiences an issue or failure, it can lead to system breakdown. This failure can disrupt the entire operation or functionality of the system, often causing system downtime or loss of critical functions. Mitigating the risk of SPOFs is crucial for maintaining the reliability and continuity of systems and infrastructure. Strategies like redundancy and failover mechanisms are commonly employed to address and minimize the impact of SPOFs, ensuring that essential services or processes can continue even in the presence of potential failure points.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;That&amp;rsquo;s the definition from ChatGPT. Our definition is more loose and hand-wavy. SPOF is a component which failed, it would disrupt the scenario in concern. Roughly speaking, there&amp;rsquo;s no failover for the SPOF. A common pattern we have for IPC is to have a load balancer in front of a cluster of servers. Our service would target this load balancer, in our case an Azure Traffic Manager, when calling other services. This is also true for other services calling our service. In this case, while if a single server host goes down, the load balancer would take this host out of rotation. This is all for stateless services, but for stateful services, we need to worry about persisting and retrieving session data. Let&amp;rsquo;s shelf this topic for now. With this pattern, while a single server is not, the load balancer is self is a SPOF. If the load balancer goes down, the whole cluster is inaccessible, there&amp;rsquo;s no failover and the scenario is disrupted.&lt;/p&gt;
&lt;p&gt;This seems inevitable for server load balancing. That is, from the client point of view, it wholly delegates the balancing task to another service, the load balancer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../20231013170530.png&#34; alt=&#34;Server side load balancing&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;blast-radisus&#34;&gt;Blast radisus&lt;/h3&gt;
&lt;details&gt;
    &lt;summary&gt;Definition of Blast Radius&lt;/summary&gt;
    &lt;p&gt;The term &#34;blast radius&#34; refers to the potential impact or extent of damage that can occur as a result of a security breach, system failure, or other adverse events within a network or system. It is often used in the context of cybersecurity and risk assessment to describe how far-reaching the consequences of a particular incident could be.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Again, that might be formal definition. For us, it&amp;rsquo;s the impact of an outage. For example, if a service is down, how many other services or regions would be affected. For example, our service is global, if our partner service goes down in a region, how would we contain the impact to be within the region?&lt;/p&gt;
&lt;p&gt;With server load balancing, this problem seems trivial. Assume that we can detect a problem through, for example, interval health check, then we can take the unhealthy hosts out of rotations. The blast radius will be contained within the problematic hosts. So far we only discuss availability aspect, one could further argue on data integrity or service reliability. Let&amp;rsquo;s shelf those for now.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Unlike in academia, people in a meeting often forego the basics, and assume knowledge that others might not have. At some companies, where design reviews are taken seriously (you can tell my opinion on mine), people would often ask for clarification, design docs and guidance are written with standards.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Can we assume that audience know what micro-service is?&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;for-your-boss-ask-what-was-the-outcome&#34;&gt;For your boss&amp;rsquo; ask, what was the outcome?&lt;/h2&gt;
&lt;p&gt;My team engineering culture has room (lots) for improvement. From my observations, it could be a number of reasons, budget constraint that led to overwork, subpar work quality to meet deadline. One major one, I dare not input, is mismanagement. The larger organization seems disconnected, guidance are generic and become obsolete quickly, initiatives ended up half-baked. This effort is case in point.&lt;/p&gt;
&lt;p&gt;As I pointed out earlier, we identified the issues. However, this is a problem for every other services. Let&amp;rsquo;s wait for them to fix it, or come up with a solution. That was the last update I have for this initiative, we then shelf this discussion. Perhaps, we can revisit this in the future where it has some business impact.&lt;/p&gt;
&lt;p&gt;Enough for the rant and negativity, I promise the next section would be more positive.&lt;/p&gt;
&lt;h2 id=&#34;the-solution&#34;&gt;The solution&lt;/h2&gt;
&lt;h3 id=&#34;my-companys-approach&#34;&gt;My company&amp;rsquo;s approach&lt;/h3&gt;
&lt;p&gt;One of the partner service team came up with a pattern for request hedging. It&amp;rsquo;s an addition to the server load balancing pattern. Instead of sending one request to the global traffic manager endpoint, their service sends 2 requests to primary and secondary endpoints with some jitter in between. Once one respond came back, it would cancel the other pending request. The advantage for this solution is that the IPC client is still fairly simple to implement, latency is kept at the lesser of the 2 requests. Most importantly, we solve the SPOF as indicated above. The cost is that we will always have 1 redudant request, and complexity added into traffic distribution pattern. The latter might be problematic if services are not scaled sufficiently in different regions. Also, why would service A need to know about service B&amp;rsquo;s internal capacity planning?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../20231013171459.png&#34; alt=&#34;Request hedging&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;lucky-find-on-the-internet&#34;&gt;Lucky find on the internet&lt;/h3&gt;
&lt;p&gt;One day I was looking at techblog articles and found &lt;a href=&#34;https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51&#34;&gt;Zero Configuration Service Mesh With On Demand Cluster Discovery&lt;/a&gt;. Admittedly, I don&amp;rsquo;t have the persuasion skill to one day convince my bosses that this is worth adopting, maybe if we improve our engineering culture, but that&amp;rsquo;s a topic for another day.&lt;/p&gt;
&lt;p&gt;The article is on how netflix adopted &lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;Envoy&lt;/a&gt;, an OSS project created by Lyft. At the center of the solution is client load balancing. The main addition they had is the ability to on-demand discover the cluster of hosts, which does not require initial configurations. Of course, this comes with some limitation, but they all have an easy solution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../20231016153620.png&#34; alt=&#34;Client load balancing&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s discuss the relevancy for this post. This indeed solves the SPOF, the &amp;ldquo;balancing&amp;rdquo; act is determined by a Service Discovery service, and the result of this computation is then loaded on the IPC client. If the Service Discovery service is down, the client can still work with the LKG (last known good) result.&lt;/p&gt;
&lt;p&gt;Compared with the request hedging pattern above, we only need to make 1 request. The burden of knowing where to best route traffic is on the Service Discovery side. Service B can send feedback to Service Discovery so that complex balancing based on capacity planning, current service health are considered.&lt;/p&gt;
&lt;p&gt;One more addition that is not yet mentioned on the netflix article is how would we manage routing by proximity. How would service A send a request to an instance of service B with the closest proximity, either in terms of network hop or physical instance. I think this work can be done by either maintaining a graph on the service discover side, or on the client side depending on the contract for the IPC client. I have a feeling, it&amp;rsquo;s easier if it&amp;rsquo;s managed outside of the client, if you know better, send me a text.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a long way to adopt this architecture, but partially implement the pieces or framework is also valuable and a good investment for us. I understand this level of optimization is great to discuss but does not have great value/effort ratio in most business cases. But again, that&amp;rsquo;s what this platform is for, right?&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
