<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on hanknguyen</title>
    <link>/posts.html</link>
    <description>Recent content in Posts on hanknguyen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>hanknguyen {year}</copyright>
    <lastBuildDate>Mon, 09 Oct 2023 15:52:20 -0700</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Client Load Balancing to Boost Service Availability</title>
      <link>/posts/client-load-balance.html</link>
      <pubDate>Mon, 09 Oct 2023 15:52:20 -0700</pubDate>
      
      <guid>/posts/client-load-balance.html</guid>
      <description>Elevating Service Availability and Banishing Single Points of Failure Earlier this year, my boss entrusted me with the task of assessing the impact of outages and identifying any potential single points of failure in our workflow. What I discovered was that every interaction my service had with other services, whether outbound or inbound, constituted a single point of failure. While the micro-service world often uses the term Inter-Process Communication (IPC) to describe this, my team prefers not to be bound by such jargon.</description>
      <content>&lt;h1 id=&#34;elevating-service-availability-and-banishing-single-points-of-failure&#34;&gt;Elevating Service Availability and Banishing Single Points of Failure&lt;/h1&gt;
&lt;p&gt;Earlier this year, my boss entrusted me with the task of assessing the impact of outages and identifying any potential single points of failure in our workflow. What I discovered was that every interaction my service had with other services, whether outbound or inbound, constituted a single point of failure. While the micro-service world often uses the term Inter-Process Communication (IPC) to describe this, my team prefers not to be bound by such jargon. Sometimes, it feels like my team operates in its own orbit, but that&amp;rsquo;s not the focus of this post.&lt;/p&gt;
&lt;h2 id=&#34;unpacking-the-terminology&#34;&gt;Unpacking the Terminology&lt;/h2&gt;
&lt;h3 id=&#34;single-point-of-failure&#34;&gt;Single Point of Failure&lt;/h3&gt;
&lt;details&gt;
    &lt;summary&gt;Defining a Single Point of Failure (SPOF)&lt;/summary&gt;
    &lt;p&gt;A Single Point of Failure (SPOF) is a critical component within a system or network. If it encounters an issue or failure, it can trigger a system breakdown, disrupting its operation and functionality. This often results in system downtime or a loss of critical functions. To mitigate the risk of SPOFs, it&#39;s vital to employ strategies like redundancy and failover mechanisms. These approaches ensure that essential services or processes can continue even in the face of potential failure points.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;That&amp;rsquo;s the definition from ChatGPT. Our interpretation is more laid-back. To us, a SPOF is any element whose failure disrupts the scenario at hand. In simpler terms, there&amp;rsquo;s no backup plan for the SPOF. When it comes to IPC, our common approach involves placing a load balancer in front of a server cluster. Our service directs requests to this load balancer, which, in our case, is an Azure Traffic Manager when interacting with other services. The same applies to other services calling our own. In this setup, if a single server host falters, the load balancer removes it from rotation. For stateless services, this mechanism works seamlessly. However, for stateful services, concerns arise about persisting and retrieving session data. Let&amp;rsquo;s set this topic aside for now. With our pattern, the server isn&amp;rsquo;t the SPOF, but the load balancer itself is. If the load balancer fails, the entire cluster becomes unreachable, with no failover, and the scenario is disrupted.&lt;/p&gt;
&lt;p&gt;This situation is somewhat inevitable with server load balancing. From the client&amp;rsquo;s perspective, it entrusts the balancing task to another service—the load balancer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../20231013170530.png&#34; alt=&#34;Server-Side Load Balancing&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;blast-radius&#34;&gt;Blast Radius&lt;/h3&gt;
&lt;details&gt;
    &lt;summary&gt;Defining Blast Radius&lt;/summary&gt;
    &lt;p&gt;The term &#34;blast radius&#34; refers to the potential impact or extent of damage that can result from a security breach, system failure, or other adverse events within a network or system. It&#39;s often used in the context of cybersecurity and risk assessment to describe how far-reaching the consequences of a particular incident could be.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Again, this might be the formal definition, but for us, it&amp;rsquo;s all about understanding the impact of an outage. For example, if a service goes down, we want to know how many other services or regions would be affected. Since our service operates globally, containing the impact within a specific region if our partner service faces an issue is crucial.&lt;/p&gt;
&lt;p&gt;With server load balancing, solving this problem seems straightforward. Suppose we can detect issues through, for instance, interval health checks. In that case, we can swiftly remove unhealthy hosts from rotation, ensuring that the blast radius remains limited to the problematic hosts. So far, we&amp;rsquo;ve only discussed availability; additional considerations like data integrity or service reliability are topics for another day.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In contrast to academia, meetings often skip the basics and assume everyone shares the same knowledge. In some companies where design reviews are taken seriously (you can probably guess my opinion on my own organization), people are encouraged to ask for clarification. Design documents and guidelines are crafted with clear standards.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Can we assume that the audience knows what a micro-service is?&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;so-what-was-the-outcome-of-your-bosss-inquiry&#34;&gt;So, What Was the Outcome of Your Boss&amp;rsquo;s Inquiry?&lt;/h2&gt;
&lt;p&gt;My team&amp;rsquo;s engineering culture leaves room for improvement. From my observations, several factors contribute to this, from budget constraints leading to overwork and subpar quality to meet deadlines. There&amp;rsquo;s a major concern that I hesitate to voice, which could be mismanagement. The larger organization often appears disjointed, with guidance quickly becoming outdated and initiatives ending up half-baked. This particular effort serves as a prime example.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier, we&amp;rsquo;ve identified the issues. However, this isn&amp;rsquo;t just our problem; it affects every other service too. For now, we&amp;rsquo;re opting to let them address it or devise a solution. That was the latest update I have on this initiative, and we&amp;rsquo;ve temporarily shelved the discussion. Perhaps we can revisit it in the future when it holds more significant business implications.&lt;/p&gt;
&lt;p&gt;Enough of the rant and negativity; I promise the next section will be more positive.&lt;/p&gt;
&lt;h2 id=&#34;the-solution&#34;&gt;The Solution&lt;/h2&gt;
&lt;h3 id=&#34;our-companys-approach&#34;&gt;Our Company&amp;rsquo;s Approach&lt;/h3&gt;
&lt;p&gt;One of the partner service teams came up with a pattern for request hedging, an addition to the server load balancing approach. Instead of sending a single request to the global traffic manager endpoint, their service sends two requests to primary and secondary endpoints, with some jitter in between. As soon as one response is received, it cancels the other pending request. This solution retains a relatively simple implementation for the IPC client, maintaining low latency by choosing the faster of the two requests. Most importantly, it addresses the SPOF issue mentioned earlier. However, it does introduce the cost of one redundant request and added complexity to the traffic distribution pattern. The latter could be problematic if services aren&amp;rsquo;t adequately scaled across different regions. Moreover, why should service A concern itself with service B&amp;rsquo;s internal capacity planning?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../20231013171459.png&#34; alt=&#34;Request Hedging&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;a-lucky-discovery-on-the-internet&#34;&gt;A Lucky Discovery on the Internet&lt;/h3&gt;
&lt;p&gt;One day, while perusing tech blog articles, I stumbled upon &lt;a href=&#34;https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51&#34;&gt;Zero Configuration Service Mesh With On-Demand Cluster Discovery&lt;/a&gt;. Admittedly, I don&amp;rsquo;t possess the persuasive skills to convince my superiors that adopting this solution is worth the effort—perhaps if we enhance our engineering culture, but that&amp;rsquo;s a topic for another day.&lt;/p&gt;
&lt;p&gt;The article details how Netflix embraced &lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;Envoy&lt;/a&gt;, an open-source project developed by Lyft. At the heart of their solution is client load balancing. Their main innovation is the ability to dynamically discover a host cluster, eliminating the need for initial configurations. Of course, there are limitations, but they all come with straightforward solutions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../20231016153620.png&#34; alt=&#34;Client Load Balancing&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s discuss the relevance of this discovery for our post. This solution indeed banishes the SPOF issue. The &amp;ldquo;balancing&amp;rdquo; act is determined by a Service Discovery service, with the results loaded onto the IPC client. Even if the Service Discovery service encounters downtime, the client can still function using the last known good result.&lt;/p&gt;
&lt;p&gt;Compared to the request hedging pattern mentioned earlier, this solution streamlines the process, requiring just one request. The responsibility of determining the best route for traffic lies with the Service Discovery side. Service B can provide feedback to the Service Discovery service, allowing complex balancing based on capacity planning and the current service&amp;rsquo;s health.&lt;/p&gt;
&lt;p&gt;One aspect not covered in the Netflix article is how to manage routing by proximity. How can service A send a request to a service B instance that&amp;rsquo;s closest in proximity, be it in terms of network hops or physical location? This task could be handled by maintaining a graph on the service discovery side or on the client side, depending on the IPC client&amp;rsquo;s contract. I have a hunch that it&amp;rsquo;s easier to manage this outside the client, but if you have better insights, feel free to reach out.&lt;/p&gt;
&lt;p&gt;The journey to adopting this architecture is long, but even implementing parts or a framework can be highly valuable and a worthwhile investment for us. I acknowledge that this level of optimization might be more of an intellectual exercise, offering limited value relative to the effort in most business cases. But then again, that&amp;rsquo;s what this platform is for, right?&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Simple Static Site Generateor: Writing Made Easy!</title>
      <link>/posts/blog-framework.html</link>
      <pubDate>Sun, 02 Jul 2023 13:54:43 -0700</pubDate>
      
      <guid>/posts/blog-framework.html</guid>
      <description>A Simple Static Site Generator: Writing Made Easy! In a world full of Content Management Systems (CMS) like WordPress and Wix, where most solutions rely on backends, I found an exception – Hugo. In this blog, I&amp;rsquo;ll walk you through my journey of implementing a straightforward framework that doesn&amp;rsquo;t require coding expertise.
The Lightbulb Moment My goal was to have a lightweight system that allowed me to focus on writing without the hassle of backend servers and associated costs.</description>
      <content>&lt;h1 id=&#34;a-simple-static-site-generator-writing-made-easy&#34;&gt;A Simple Static Site Generator: Writing Made Easy!&lt;/h1&gt;
&lt;p&gt;In a world full of Content Management Systems (CMS) like WordPress and Wix, where most solutions rely on backends, I found an exception – &lt;a href=&#34;https://gohugo.io/showcase/&#34;&gt;Hugo&lt;/a&gt;. In this blog, I&amp;rsquo;ll walk you through my journey of implementing a straightforward framework that doesn&amp;rsquo;t require coding expertise.&lt;/p&gt;
&lt;h2 id=&#34;the-lightbulb-moment&#34;&gt;The Lightbulb Moment&lt;/h2&gt;
&lt;p&gt;My goal was to have a lightweight system that allowed me to focus on writing without the hassle of backend servers and associated costs. While WordPress does offer free options, they often come with complex setups. Little did I know about &lt;a href=&#34;https://gohugo.io/showcase/&#34;&gt;Hugo&lt;/a&gt;, a static site generator that simplifies website creation. I may have been reinventing the wheel, but the learning experience was invaluable.&lt;/p&gt;
&lt;h3 id=&#34;my-toolbox&#34;&gt;My Toolbox&lt;/h3&gt;
&lt;p&gt;I decided to stick with free tools for my project, making the following assumptions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GitHub Pages are free.&lt;/li&gt;
&lt;li&gt;I have a ChatGPT Plus subscription.&lt;/li&gt;
&lt;li&gt;I know my way around frontend technologies (HTML, CSS, JS).&lt;/li&gt;
&lt;li&gt;I have an established brand - &lt;a href=&#34;https://baophotos.ca/&#34;&gt;BAO Photography&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lets-get-started&#34;&gt;Let&amp;rsquo;s Get Started&lt;/h3&gt;
&lt;p&gt;My approach was to create a framework that generates static HTML pages and host them on GitHub Pages. I would also leverage ChatGPT to do most of the coding heavy lifting. Don&amp;rsquo;t worry; you don&amp;rsquo;t need to be a coding wizard to follow along.&lt;/p&gt;
&lt;h4 id=&#34;step-1-creating-a-basic-html-page&#34;&gt;Step 1: Creating a Basic HTML Page&lt;/h4&gt;
&lt;p&gt;You can start by asking ChatGPT to generate a basic HTML page with a title and a tagline. Feel free to add your own styles and components. For my blog, I included a container for the list of blog posts.&lt;/p&gt;
&lt;h4 id=&#34;step-2-presenting-blog-posts&#34;&gt;Step 2: Presenting Blog Posts&lt;/h4&gt;
&lt;p&gt;I decided to present each blog post as an HTML page. I wrote my blog posts in Markdown and used a Python script generated with ChatGPT to convert Markdown to HTML. This script also compiled the list of blog posts and generated a JSON file with post metadata.&lt;/p&gt;
&lt;h4 id=&#34;step-3-displaying-posts&#34;&gt;Step 3: Displaying Posts&lt;/h4&gt;
&lt;p&gt;To display my blog posts, I populated the container on the main page using the JSON metadata. This process happens relatively dynamically, meaning we load the JSON file and populate the HTML container when the client loads the page. To achieve this, I used iframes to display the posts.&lt;/p&gt;
&lt;h4 id=&#34;step-4-writing-blog-posts&#34;&gt;Step 4: Writing Blog Posts&lt;/h4&gt;
&lt;p&gt;I created a Powershell script that generates a new blog post file with a predefined template. The script automatically names the file and increments the post number.&lt;/p&gt;
&lt;h4 id=&#34;step-5-execute-and-publish&#34;&gt;Step 5: Execute and Publish&lt;/h4&gt;
&lt;p&gt;With everything set up, I executed my scripts and pushed the changes to my GitHub repository.&lt;/p&gt;
&lt;h2 id=&#34;in-a-nutshell&#34;&gt;In a Nutshell&lt;/h2&gt;
&lt;p&gt;This simple framework allowed me to generate static HTML pages and host them on GitHub Pages effortlessly. ChatGPT played a crucial role in automating various tasks, making my life easier. While AI is undoubtedly a powerful tool, I believe that the human touch and creativity will always have a place. It&amp;rsquo;s all about adapting and learning new skills to stay ahead in an ever-changing landscape.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../overall.png&#34; alt=&#34;Overall/&#34;&gt;&lt;/p&gt;
&lt;p&gt;So, why spend days on complex setups when you can create your website in an evening? Get creative, write, and let AI do the heavy lifting – it&amp;rsquo;s a win-win situation! 😄&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Exploring the Journey of a Read-Optimized Service</title>
      <link>/posts/read-optimized-service.html</link>
      <pubDate>Sun, 02 Jul 2023 13:54:43 -0700</pubDate>
      
      <guid>/posts/read-optimized-service.html</guid>
      <description>The Balancing Act in Crafting a Read-Optimized Service In the world of software engineering, I find myself immersed in the realm of a read-optimized service. Today, I&amp;rsquo;m excited to share some of the fascinating trade-offs that we&amp;rsquo;ve encountered while designing this service. Along the way, I&amp;rsquo;ll also shed light on the challenges and incidents that have shaped our journey. So, let&amp;rsquo;s embark on this insightful exploration together!
Disclaimer: I may not have the full context behind the design decisions, as they were made by the service&amp;rsquo;s previous owners.</description>
      <content>&lt;h1 id=&#34;the-balancing-act-in-crafting-a-read-optimized-service&#34;&gt;The Balancing Act in Crafting a Read-Optimized Service&lt;/h1&gt;
&lt;p&gt;In the world of software engineering, I find myself immersed in the realm of a read-optimized service. Today, I&amp;rsquo;m excited to share some of the fascinating trade-offs that we&amp;rsquo;ve encountered while designing this service. Along the way, I&amp;rsquo;ll also shed light on the challenges and incidents that have shaped our journey. So, let&amp;rsquo;s embark on this insightful exploration together!&lt;/p&gt;
&lt;p&gt;Disclaimer: I may not have the full context behind the design decisions, as they were made by the service&amp;rsquo;s previous owners. Nevertheless, I&amp;rsquo;m here to provide my perspective on these trade-offs.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;a-new-landscape&#34;&gt;A New Landscape&lt;/h3&gt;
&lt;p&gt;As of June 30, 2023, I&amp;rsquo;ve been part of my current team for a year and a half. This team stands out from my previous experiences, as we manage services and components that are relatively isolated. Unlike my previous role, where we had a tightly knit system of interconnected microservices, data flowing seamlessly, and well-defined contracts between upstream and downstream. Work planning was a breeze, and collaboration was encouraged. In my current role, it feels more natural to divide team members into specialized areas of expertise. While this ownership model has its merits, it does come with some overhead for team members to familiarize themselves with other areas. This doesn&amp;rsquo;t mean collaboration isn&amp;rsquo;t encouraged; it&amp;rsquo;s just a bit more complex. I often wonder which setting is more common in the industry, and I&amp;rsquo;d love to hear your thoughts.&lt;/p&gt;
&lt;h3 id=&#34;my-role&#34;&gt;My Role&lt;/h3&gt;
&lt;p&gt;As part of this ownership model, I was entrusted with the task of managing a particular service. Unfortunately, the previous owner left shortly after, leaving me to navigate the transition. While I would have preferred a more guided experience, my boss was supportive and gave me the space to dive into the code and make sense of the unstructured or incomplete documentation. The silver lining in this endeavor is that the challenging aspect of this service lies in its scaling, not in its business logic. The business logic is quite straightforward; it&amp;rsquo;s essentially a CRUD store! Scaling, on the other hand, is where the real excitement lies. I&amp;rsquo;ve spent two years with my previous team tackling scaling issues, and while I&amp;rsquo;m no expert, I&amp;rsquo;m immensely grateful for the knowledge I&amp;rsquo;ve gained from my mentor.&lt;/p&gt;
&lt;h2 id=&#34;the-service&#34;&gt;The Service&lt;/h2&gt;
&lt;h3 id=&#34;the-core-purpose&#34;&gt;The Core Purpose&lt;/h3&gt;
&lt;p&gt;As previously mentioned, our service is essentially a CRUD store. Without delving too deep into the intricacies of our business, I can tell you that it&amp;rsquo;s primarily geared towards read-heavy operations, handling roughly 2000 queries per second on a global scale. The p99 latency for this service typically hovers around an impressive sub-50 milliseconds.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../20230630171617.png&#34; alt=&#34;QPS&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-architectural-blueprint&#34;&gt;The Architectural Blueprint&lt;/h3&gt;
&lt;p&gt;The architecture behind this distributed service system is quite fascinating, with several crucial design decisions that have significantly shaped our journey. Let&amp;rsquo;s walk through them one by one.&lt;/p&gt;
&lt;h4 id=&#34;regional-deployment&#34;&gt;Regional Deployment&lt;/h4&gt;
&lt;p&gt;Our service is spread across multiple regions, each housing several hosts. All these hosts are meticulously placed behind regional traffic managers, ensuring optimal performance.&lt;/p&gt;
&lt;h4 id=&#34;underlying-persistent-storage&#34;&gt;Underlying Persistent Storage&lt;/h4&gt;
&lt;p&gt;We opted for a globally replicated NoSQL database, avoiding the complexity of sharding.&lt;/p&gt;
&lt;h4 id=&#34;full-in-memory-cache&#34;&gt;Full In-Memory Cache&lt;/h4&gt;
&lt;p&gt;One of our standout features is maintaining a complete copy of the database in an in-memory cache. When handling read requests, we query the cache instead of the underlying data store.&lt;/p&gt;
&lt;h4 id=&#34;cache-invalidation&#34;&gt;Cache Invalidation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Whenever we process write requests, we simultaneously record the changes in another table.&lt;/li&gt;
&lt;li&gt;Periodically, across all hosts, we read this change record table, query the main table, and update the in-memory cache accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-trade-offs&#34;&gt;The Trade-Offs&lt;/h2&gt;
&lt;p&gt;With all this groundwork in place, let&amp;rsquo;s dive into the challenges. I&amp;rsquo;ll take you on a chronological journey through the incidents and hurdles my team has faced over the years.&lt;/p&gt;
&lt;h3 id=&#34;incident-1-the-cpu-usage-surge&#34;&gt;Incident 1: The CPU Usage Surge&lt;/h3&gt;
&lt;p&gt;We faced a significant spike in traffic, which triggered a CPU usage alert. Our immediate response was to horizontally scale out the service, adding more instances to the region where the alarm had been set off.&lt;/p&gt;
&lt;h4 id=&#34;scaling-woes&#34;&gt;Scaling Woes&lt;/h4&gt;
&lt;p&gt;While our CPU usage dropped after scaling, we were soon confronted with another problem. We began to receive &amp;ldquo;429 Too Many Requests&amp;rdquo; errors from the underlying database. This incident underscores one of the perils of scaling out services: scaling down can be sticky. In the immortal words of wisdom, &amp;ldquo;If there&amp;rsquo;s no problem, maybe don&amp;rsquo;t touch it.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;discovering-the-new-bottleneck&#34;&gt;Discovering the New Bottleneck&lt;/h4&gt;
&lt;p&gt;What followed was the fan-out effect. Multiple hosts were querying the same global database instance, causing the database itself to become the bottleneck. Fortunately, we hadn&amp;rsquo;t reached the limits of scaling out the database; we had only reached the capped capacity, which we could increase. In fact, today, our database alone costs more in a month than what I earn in a year. While it may seem unfair, I can&amp;rsquo;t help but wonder if this is a common practice in the industry.&lt;/p&gt;
&lt;h4 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h4&gt;
&lt;p&gt;We realized that we needed a smarter solution for managing CPU usage issues.&lt;/p&gt;
&lt;h3 id=&#34;incident-2-i-created-a-record-but-cant-find-it&#34;&gt;Incident 2: &amp;ldquo;I Created a Record, But Can&amp;rsquo;t Find It!&amp;rdquo;&lt;/h3&gt;
&lt;p&gt;The larger organization has been on a mission to enhance service reliability. We&amp;rsquo;re aiming for high reliability metrics, specifying how many nines we want to achieve. For example, four nines translate to 99.99% reliability. Achieving this metric for our service is usually a breeze, as long as there&amp;rsquo;s no crash. The service either returns data from the cache or nothing at all, ensuring that cache misses don&amp;rsquo;t compromise reliability.&lt;/p&gt;
&lt;h4 id=&#34;the-unanswered-question&#34;&gt;The Unanswered Question&lt;/h4&gt;
&lt;p&gt;However, the catch here is that the service is eventually consistent, which can lead to slight delays.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Team: &amp;ldquo;How long ago did you create the record?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Client: &amp;ldquo;A while back.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We haven&amp;rsquo;t committed to a specific Service Level Agreement (SLA) for eventual consistency, but if it takes hours, without revealing too much about our business logic, let&amp;rsquo;s just say that it&amp;rsquo;s not acceptable.&lt;/p&gt;
&lt;h4 id=&#34;the-waiting-game&#34;&gt;The Waiting Game&lt;/h4&gt;
&lt;p&gt;Cache invalidation is typically set to occur within a 10-second interval. But the repercussions of our previous scaling efforts came back to haunt us. A surge in write traffic resulted in a cascade effect, with each write request amplifying the load on our database. At one point, nearly 80% of the traffic to the database was being throttled.&lt;/p&gt;
&lt;p&gt;For each throttled request, we attempted retries without introducing jitter, meaning that after a fixed period, all hosts would bombard the database simultaneously. This classic thundering herd problem was causing some records to go missing, as they exceeded the maximum retry attempts.&lt;/p&gt;
&lt;h4 id=&#34;a-clever-mitigation&#34;&gt;A Clever Mitigation&lt;/h4&gt;
&lt;p&gt;The solution? A good old-fashioned restart! I learned this trick from my father when I was five, and it still works like a charm. Restarting the host flushes the queue of changed records, offering a clean slate. The cache is then repopulated with the latest data from the database.&lt;/p&gt;
&lt;h4 id=&#34;lessons-learned-part-2&#34;&gt;Lessons Learned (Part 2)&lt;/h4&gt;
&lt;p&gt;We realized the importance of addressing the fan-out effect and tackling the thundering herd problem. Following the incident, we introduced jitter when retrying and transitioned the queue of changed records to a set. This change allowed us to avoid duplicate work; for instance, if record A was modified twice, we only needed to apply the change once.&lt;/p&gt;
&lt;h3 id=&#34;incidents-3-and-beyond-we-need-your-team-to-support-our-new-feature&#34;&gt;Incidents 3 and Beyond: &amp;ldquo;We Need Your Team to Support Our New Feature&amp;rdquo;&lt;/h3&gt;
&lt;p&gt;Many of the incidents related to this service were triggered by an upsurge in write requests without a corresponding scale-out of either the service hosts or the underlying datastore. As I&amp;rsquo;ve mentioned, our service predominantly handles read-heavy workloads, and our design decisions reflect this priority. However, this trade-off does present some challenges.&lt;/p&gt;
&lt;h4 id=&#34;the-communication-conundrum&#34;&gt;The Communication Conundrum&lt;/h4&gt;
&lt;p&gt;Most of these incidents could have been prevented or at least managed better if we had foreseen the increase in write traffic and scaled out the bottlenecks accordingly.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Team: &amp;ldquo;We&amp;rsquo;re launching a new feature that will increase write traffic by tenfold.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;My Team: &amp;ldquo;Alright.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But the reality of communication is rarely so clear-cut. Sometimes, we aren&amp;rsquo;t aware of the new feature; other times, we&amp;rsquo;re aware but uncertain of its impact. Communication gaps can arise, and it&amp;rsquo;s not always easy to predict how much a feature will increase traffic or whether it&amp;rsquo;s a one-time event or a recurring one. Oops, it seems like I&amp;rsquo;m repeating myself, but you get the idea.&lt;/p&gt;
&lt;h4 id=&#34;a-proposed-solution&#34;&gt;A Proposed Solution&lt;/h4&gt;
&lt;p&gt;In my view, clear Service Level Agreements (SLAs) between my team and our clients would go a long way in preventing surprises when it comes to anticipated traffic increases.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve voiced this idea a couple of times but haven&amp;rsquo;t fully implemented it yet. My apologies to my boss! I&amp;rsquo;m gradually building connections with other teams, and I hope that, in the future, there will be fewer surprises.&lt;/p&gt;
&lt;!-- intermediate --&gt;
</content>
    </item>
    
  </channel>
</rss>
